# The all target helps automate the whole process: by running `make` from the command line, you can do everything in one go
# You can define further targets that only execute smaller subsets of your data pipeline according to your needs
all: clean collect process analyze

clean:
	rm -rf data processed analysis

.PHONY: collect processed analysis adhoc
collect:
	# This target is usually associated with data collection
	# This can involved scraping websites, downloading files from servers,
	# or other similar operations.
	# As best practice, ensure that all output data goes to a known location (e.g., here, data/)
	mkdir -p data/...
	Rscript code/get_data_1.R

process:
	# This target is reserved for data processing, which typically includes
	# cleaning and refinement.
	# As best practice, have multiple scripts to perform different (sub)steps
	# You may even opt for several targets for bigger granularity
	# (e.g., a process_cleaning and a process_refinement target)
	# Moreover, ensure that data also goes to a known location for easier analysis
	mkdir -p processed/...
	Rscript code/process_data1.R
	sh code/process_data2.sh
	python3 code/process_data3.py

analyze:
	# This target is recommended to isolate all data analysis scripts.
	# Once again, it is recommended to separate different types of analysis between scripts,
	# which may span several languages. Diversity is key here so data can be better understood.
	mkdir -p analysis/...
	Rscript code/produce_some_plots.R
	go run code/do_text_analysis.go

adhoc:
	# This target is not part of the overall automation, but it can be useful to have something similar
	# to automate some less frequent operation that you might want to run only when strictly necessary
	# (e.g., organize all produced data/analysis and run a notebook for an easier visual verification of obtained results)
	Rscript code/some_adhoc_script.R
